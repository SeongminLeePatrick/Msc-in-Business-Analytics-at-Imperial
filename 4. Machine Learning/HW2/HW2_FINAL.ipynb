{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Homework 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP L\n",
    "- Josiane Dufour 01388388\n",
    "- Otto Godwin 01422474\n",
    "- Zana-Ljubica Krsticevic 01423239\n",
    "- Shuyu Zhou 01382817\n",
    "- Seongmin Lee 01247436"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data into a Python data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import module\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import functools as ft\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from re import sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                                                  1\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_table(\"SMSSpamCollection.txt\", header=None) \n",
    "# make sure to put the '.txt' file in the work directory and name it correctly\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Pre-process the SMS messages: Remove all punctuation and numbers from the SMS messages, and change all messages to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go until jurong point crazy available only in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say so early hor u c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  go until jurong point crazy available only in ...\n",
       "1   ham                            ok lar joking wif u oni\n",
       "2  spam  free entry in a wkly comp to win fa cup final ...\n",
       "3   ham        u dun say so early hor u c already then say\n",
       "4   ham  nah i don t think he goes to usf he lives arou..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = [\"type\", \"text\"]\n",
    "r = '[^A-Za-z\\s]+'\n",
    "text = data.iloc[:,1].apply(lambda x: re.sub(r, \"\", x).lower().strip())\n",
    "data.text = text\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Shuffle the messages and split them into a training set (2,500 messages), a validation set (1,000 messages) and a test set (all remaining messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffle dataset\n",
    "data = shuffle(data, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train, validation and test dataset\n",
    "train_set = data.iloc[0:2500,:]\n",
    "validation_set = data.iloc[2500:3500,:]\n",
    "test_set = data.iloc[3500:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spam = list(train_set.loc[data.type == 'spam', :].text)\n",
    "train_ham = list(train_set.loc[data.type == 'ham', :].text)\n",
    "train_message = list(train_set.text)\n",
    "train_label = list(train_set.type)\n",
    "validation_message = list(validation_set.text)\n",
    "validation_label = list(validation_set.type)\n",
    "test_message = list(test_set.text)\n",
    "test_label = list(test_set.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Build Naive Bayes Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NaiveBayesForSpam:\n",
    "    def train(self, hamMessages, spamMessages) :\n",
    "        self.words = set (' '.join(hamMessages + spamMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors [0] = float(len(hamMessages)) / (len(hamMessages)+len(spamMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i , w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m]) ) / len(spamMessages)\n",
    "            self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "        \n",
    "    def train2 (self, hamMessages, spamMessages) :\n",
    "        self.words = set (\" \".join(hamMessages + spamMessages).split())\n",
    "        self.priors = np. zeros (2)\n",
    "        self.priors[0] = float (len(hamMessages)) / (len(hamMessages) + len(spamMessages))\n",
    "        self.priors [1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = [ ]\n",
    "        spamkeywords = [ ]\n",
    "        for i , w in enumerate(self.words):\n",
    "            prob1 = (1.0 + len([m for m in hamMessages if w in m])) / len(hamMessages)\n",
    "            prob2 = (1.0 + len([m for m in spamMessages if w in m])) / len(spamMessages)\n",
    "            if prob1 * 20 < prob2 :\n",
    "                self.likelihoods.append([min(prob1, 0.95), min(prob2, 0.95)])\n",
    "                spamkeywords.append(w)\n",
    "        self.words = spamkeywords\n",
    "        self.likelihoods = np.array(self.likelihoods).T\n",
    "        \n",
    "    def predict(self, message) :\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i , w in enumerate (self.words) :\n",
    "            if w in message.lower() : # convert to lowerâˆ’case, delete it\n",
    "                posteriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors / np. linalg.norm(posteriors, ord = 1) # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]\n",
    "    \n",
    "    def score(self, messages, labels) :\n",
    "        confusion = np.zeros(4).reshape(2 ,2)\n",
    "        for m, l in zip (messages, labels) :\n",
    "            if self.predict(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0 ,0] += 1\n",
    "            elif self.predict(m)[0] == \"ham\" and l == \"spam\":\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == \"spam\" and l == \"ham\":\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == \"spam\" and l == \"spam\":\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion\n",
    "    \n",
    "    # define new function to reduce the false positives\n",
    "    def predict2(self, message) :\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i , w in enumerate (self.words) :\n",
    "            if w in message.lower() : # convert to lowerâˆ’case, delete it\n",
    "                posteriors *= self.likelihoods[:, i]\n",
    "            else:\n",
    "                posteriors *= np.ones(2) - self.likelihoods[:, i]\n",
    "            posteriors = posteriors / np. linalg.norm(posteriors, ord = 1) # normalise\n",
    "        if posteriors[0] > 0.2:\n",
    "            return ['ham', posteriors[0]]\n",
    "        return ['spam', posteriors[1]]\n",
    "    \n",
    "    def score2(self, messages, labels) :\n",
    "        confusion = np.zeros(4).reshape(2 ,2)\n",
    "        for m, l in zip (messages, labels) :\n",
    "            if self.predict2(m)[0] == 'ham' and l == 'ham':\n",
    "                confusion[0 ,0] += 1\n",
    "            elif self.predict2(m)[0] == \"ham\" and l == \"spam\":\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict2(m)[0] == \"spam\" and l == \"ham\":\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict2(m)[0] == \"spam\" and l == \"spam\":\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float(confusion.sum()), confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What is the purpose of each function? What do â€™trainâ€™ and â€˜train2â€™ do, and what is the difference between them? Where in the code is Bayesâ€™ Theorem being applied?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Both functions 'train' and 'train2' functions calculate the prior probability of $P(HAM)$ and $P(SPAM)$ and the likelihood of $P(word \\mid HAM)$ and $P(word \\mid SPAM)$ based on a lists of words in the 'hamMessages' and the 'spamMessages'. Moreover, both functions use the Laplace Estimator in the loop, changing all probabilities of zero to 0.05. By doing so, we could sort out HAM and SPAM by comparing the $P(HAM \\mid new word)$ = priors[0]$\\times$prob_1 to $P(SPAM \\mid new word)$ = priors[1]$\\times$prob_2 in the 'predict' function. The 'score' function calculates the confusion matrix and accuracy by comparing the classifications of each new message with the original labels.\n",
    "\n",
    "- The difference between train and train2 is mainly in how it treats words classified as spam words. The line of code ```if prob1$\\times$20 < prob2``` assigns a higher weight to the likelihood of a message being 'HAM' by multiplying it by 20. As a result, the spam keywords in 'train2' have a much higher likelihood of appearing in a spam message. \n",
    "\n",
    "\n",
    "- Bayes' Theorem is applied in the predict and predict2 functions. The `posteriors` variable stores the proportional probabilities that the new message is ham and spam respectively. This is calculated by multiplying the prior of ham and spam (self.priors) by the likelihood of the message given that it is ham and spam. This is the application of Bayes' Theorem. Take for example the probability that a new message is ham (where spamword1, spamword3...spamwordn is in new message, but spamword2 is not): $P(HAM \\mid word_1 \\cap not word_2 \\cap word_3..... \\cap word_n)$. By Bayes' Theorem, the code breaks up this probability into likelihood*prior = $P(word_1 \\mid HAM)$$\\times$$P(not word_2 \\mid HAM)$$\\times$$P(word_3 \\mid HAM)$$\\times$...$\\times$$P(word_n \\mid HAM)$$\\times$$P(HAM)$. Note that above, we have broken the likelihood up further by making the Naive Bayes class conditional independence assumption. In the code this product of likelihoods and priors is computed with a simple for-loop that iterates through all spam key words and updates `posteriors` at each spam key word. \n",
    "\n",
    "- These posterior probabilites are only proportional to the true probabilities as we have ignored the marginal probability of the new message. Therefore we must scale these probabilities by normalization as though we would still get the same classification, the probabilites would become very small which would cause programming issues. Now with our threshold for classification, we can predict whether the new sentence is spam or ham. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use your training set to train the classifiers â€˜trainâ€™ and â€˜train2â€™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 'NB_classifer' use train\n",
    "NB_classifer = NaiveBayesForSpam()\n",
    "NB_classifer.train(hamMessages=train_ham, spamMessages=train_spam)\n",
    "\n",
    "# Model 'NB_classifer2' use train2\n",
    "NB_classifer2 = NaiveBayesForSpam()\n",
    "NB_classifer2.train2(hamMessages=train_ham, spamMessages=train_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train_score_train = NB_classifer.score(messages = train_message, labels = train_label)\n",
    "print(\"Running time for 'train'  function through train set     : %.4s seconds\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "train_score_train2 = NB_classifer2.score(messages = train_message, labels = train_label)\n",
    "print(\"Running time for 'train2' function through train set     : %.4s seconds\\n\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "val_score_train = NB_classifer.score(messages = validation_message, labels = validation_label)\n",
    "print(\"Running time for 'train'  function through validation set: %.4s seconds\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "val_score_train2 = NB_classifer2.score(messages = validation_message, labels = validation_label)\n",
    "print(\"Running time for 'train2' function through validation set: %.4s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'train' function through the training set\n",
      "\n",
      "Accuracy: 0.9656\n",
      "\n",
      "************Confusion_Matrix************\n",
      "                 Actual_HAM  Actual_SPAM\n",
      "Predicted_HAM       2098.0         31.0\n",
      "Predicted_SPAM        55.0        316.0\n"
     ]
    }
   ],
   "source": [
    "result_train = train_score_train\n",
    "accuracy_train, confusion_mat_train = result_train\n",
    "confusion_mat_train = pd.DataFrame(confusion_mat_train)\n",
    "confusion_mat_train.columns = ['Actual_HAM', 'Actual_SPAM']\n",
    "confusion_mat_train.index = ['Predicted_HAM', 'Predicted_SPAM']\n",
    "print(\"Result for 'train' function through the training set\\n\")\n",
    "print(\"Accuracy: %.4f\\n\" %(accuracy_train))\n",
    "print(\"************Confusion_Matrix************\\n\", confusion_mat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'train2' function through the training set\n",
      "\n",
      "Accuracy: 0.9752\n",
      "\n",
      "************Confusion_Matrix************\n",
      "                 Actual_HAM  Actual_SPAM\n",
      "Predicted_HAM       2145.0         54.0\n",
      "Predicted_SPAM         8.0        293.0\n"
     ]
    }
   ],
   "source": [
    "result_train2 = train_score_train2\n",
    "accuracy_train2, confusion_mat_train2 = result_train2\n",
    "confusion_mat_train2 = pd.DataFrame(confusion_mat_train2)\n",
    "confusion_mat_train2.columns = ['Actual_HAM', 'Actual_SPAM']\n",
    "confusion_mat_train2.index = ['Predicted_HAM', 'Predicted_SPAM']\n",
    "print(\"Result for 'train2' function through the training set\\n\")\n",
    "print(\"Accuracy: %.4f\\n\" %(accuracy_train2))\n",
    "print(\"************Confusion_Matrix************\\n\", confusion_mat_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'train' function through the validation set\n",
      "\n",
      "Accuracy: 0.9680\n",
      "\n",
      "************Confusion_Matrix************\n",
      "                 Actual_HAM  Actual_SPAM\n",
      "Predicted_HAM        858.0         13.0\n",
      "Predicted_SPAM        19.0        110.0\n"
     ]
    }
   ],
   "source": [
    "result_val= val_score_train\n",
    "accuracy_val, confusion_mat_val = result_val\n",
    "confusion_mat_val = pd.DataFrame(confusion_mat_val)\n",
    "confusion_mat_val.columns = ['Actual_HAM', 'Actual_SPAM']\n",
    "confusion_mat_val.index = ['Predicted_HAM', 'Predicted_SPAM']\n",
    "print(\"Result for 'train' function through the validation set\\n\")\n",
    "print(\"Accuracy: %.4f\\n\" %(accuracy_val))\n",
    "print(\"************Confusion_Matrix************\\n\", confusion_mat_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'train2' function through the validation set\n",
      "\n",
      "Accuracy: 0.9740\n",
      "\n",
      "************Confusion_Matrix************\n",
      "                 Actual_HAM  Actual_SPAM\n",
      "Predicted_HAM        874.0         23.0\n",
      "Predicted_SPAM         3.0        100.0\n"
     ]
    }
   ],
   "source": [
    "result_val2 = val_score_train2\n",
    "accuracy_val2, confusion_mat_val2 = result_val2\n",
    "confusion_mat_val2 = pd.DataFrame(confusion_mat_val2)\n",
    "confusion_mat_val2.columns = ['Actual_HAM', 'Actual_SPAM']\n",
    "confusion_mat_val2.index = ['Predicted_HAM', 'Predicted_SPAM']\n",
    "print(\"Result for 'train2' function through the validation set\\n\")\n",
    "print(\"Accuracy: %.4f\\n\" %(accuracy_val2))\n",
    "print(\"************Confusion_Matrix************\\n\", confusion_mat_val2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  Using the validation set, explore how each of the two classifiers performs out of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy of classifications from the 'train2' function is 97.4%, which is 0.6% higher than classifications using the 'train' function on the same validation set. In the confusion matrix, we can see that there are more true positives (i.e. actual ham messages predicted as ham messages) and less false positives (i.e. actual ham messages predicted as spam messages). However, the 'train2' function has worse performance when it comes to predicting spam messages (classifying the actual spam messages as predicted spam messages), which is the trade-off between 'train' and 'train2'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Why is the â€˜train2â€™ classifier faster? Why does it yield a better accuracy both on the training and the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of words through NB_classifier: 5157\n",
      "The length of words through NB_classifier2: 198\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of words through NB_classifier:\", len(NB_classifer.words))\n",
    "print(\"The length of words through NB_classifier2:\", len(NB_classifer2.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Running time for 'train'  function through train set     : 309. seconds\n",
    "- Running time for 'train2' function through train set     : 11.4 seconds\n",
    "- Running time for 'train'  function through validation set: 113. seconds\n",
    "- Running time for 'train2' function through validation set: 4.02 seconds\n",
    "\n",
    "\n",
    "- The 'train2' classifier is approximately 30 times faster than the 'train1 because the 'predict' function makes use of the list of \"spam key words\" from the 'train2' function, which satisfies the condition on $P(word \\mid HAM)\\times20   < P(word \\mid SPAM)$, which makes a lot smaller a set of the words (198) for filtering SPAM. On the other hand, the classifier based on the 'train' function has to use the whole list of SPAM and HAM words (5157), which takes  30 times longer to train.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.  How many false positives did you get in your validation set? How would you change the code to reduce false positives at the expense of possibly having more false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# False positves for train2 at threshold = 0.5\n",
    "fp_train2 = val_score_train2[1][1,0]\n",
    "print(int(fp_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_score_train2_reduced = NB_classifer2.score2(messages = validation_message, labels = validation_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# False positives after decreasing threshold to 0.3\n",
    "fp_train2_reduced = val_score_train2_reduced[1][1,0]\n",
    "print(int(fp_train2_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the 'train2' classifier on the test set is 96.96%, which is 0.44% lower than the accuracy of the 'train2' classifier on the validation set. The percentage of true positives is a bit higher in the validation set (87.4% compared to 86.29% in the test set). However, true negatives are better predicted on the test set (0.67% higher). All in all, the outcome from the test set is similar to the outcome from the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Run the â€˜train2â€™ classifier on the test set and report its performance using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_test = NB_classifer2.score(messages = test_message, labels = test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for 'train2' function through the test set\n",
      "\n",
      "Accuracy: 0.9696\n",
      "\n",
      "************Confusion_Matrix************\n",
      "                 Actual_HAM  Actual_SPAM\n",
      "Predicted_HAM       1788.0         56.0\n",
      "Predicted_SPAM         7.0        221.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_test, confusion_mat_test = result_test\n",
    "confusion_mat_test = pd.DataFrame(confusion_mat_test)\n",
    "confusion_mat_test.columns = ['Actual_HAM', 'Actual_SPAM']\n",
    "confusion_mat_test.index = ['Predicted_HAM', 'Predicted_SPAM']\n",
    "print(\"Result for 'train2' function through the test set\\n\")\n",
    "print(\"Accuracy: %.4f\\n\" %(accuracy_test))\n",
    "print(\"************Confusion_Matrix************\\n\", confusion_mat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy of classifications assigned using the 'train2' classfier on the test set is 96.96%, which is 0.44% lower than the 'train2' classifier on the validation set. Furthermore, there are only 7 messages classified as false positives in the confusion matrix, which is a similiar outcome from the validation set. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
