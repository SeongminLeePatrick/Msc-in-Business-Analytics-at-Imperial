{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction of SurPRISE Package (Simple Python RecommendatIon System Engine)\n",
    "\n",
    "For Question 2 Netflix Dataset, we utilized the recommendation system package called \"SurPRISE\", which has a set of built-in recommendation algorithms based on matrix factorization and collaborative filtering. In particular, the recommendation system using matrix factorization throughout the truncated SVD decomposition outputs a much better performance in the prediction. On top of that, the large size matrix in a format of users-items(m x n) scales down to user factor(U: m x k) x latent factor(sigma: k x k) x item factor (V: k x n), where the value of k is much smaller than m and n. Moreover, the small k distinctively enhances the computational efficiency. In other words, the downsized matrix factorization is the approximation of the original matrix. To minimize the error between the approximation and the original matrix in excluding the null values in the original matrix, the stochastic gradient descent (SGD) method is applied to this package. The SGD in the package is mainly implemented as following.\n",
    "\n",
    "Step1. Determine the latent factor to generate U and V <br>\n",
    "Step2. Randomly select one element from the users-items matrix <br>\n",
    "Step3. Measure the error term (adding the regularization term to avoid overfitting) <br>\n",
    "Step4. Update U and V for the minimization of the error term <br>\n",
    "Step5. Repeat Steps 2 to 4 until the error term is satisfied <br>\n",
    "\n",
    "Therefore, we need to tune parameters such as learning rate, regularization coefficient, and number of latent factors to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset, SVD, evaluate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import SVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Divide the train dataset into trainset and testset\n",
    "Before predicting the \"Netflix_HW3_test\" without the dependent label, we initially divided the trainset into a trainset and testset. By using the testset with the rating values, we can evaluate the regression model that was built in the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "df_train_raw = pd.read_csv('./netflix_HW3/Netflix_HW3_training.txt', header = None, names = ['itemID', 'userID', 'rating'])\n",
    "df_test_raw = pd.read_csv('./netflix_HW3/Netflix_HW3_test.txt', header = None, names = ['itemID', 'userID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the df_train_raw into train(80%) and test(20%)\n",
    "trainset, testset = train_test_split(df_train_raw, test_size=0.2, random_state= 135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#fit the dataset to the surprise package format\n",
    "reader = Reader(rating_scale=(1, 5)) # Rating range is 1 to 5\n",
    "train_sdf = Dataset.load_from_df(trainset[['itemID', 'userID', 'rating']], reader=reader)\n",
    "test_sdf = Dataset.load_from_df(testset[['itemID', 'userID', 'rating']], reader=reader)\n",
    "ttl_train_sdf = Dataset.load_from_df(df_train_raw[['itemID', 'userID', 'rating']], reader=reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "As mentioned above, the tuning hyper-parameters have been set as below.    \n",
    "n_factors: The number of factors. Default is 100. <br>\n",
    "n_epochs: The number of iterations of the SGD procedure. Default is 20. <br>\n",
    "lr_all: The learning rate for all parameters. Default is 0.005. <br>\n",
    "reg_all: The regularization term for all parameters. Default is 0.02. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameter setting\n",
    "param_grid = {'n_factors':[60, 90, 120], 'n_epochs': [20, 30, 40], 'lr_all': [0.003, 0.005, 0.007],\n",
    "              'reg_all': [0.01, 0.02, 0.03]}\n",
    "\n",
    "SVD_Grid = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit the data\n",
    "SVD_Grid.fit(train_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Best RSME Score====\n",
      " 0.8473964427371665\n",
      "\n",
      "==== Hyper-parameter Values for the Best RMSE====\n",
      " {'n_factors': 60, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.03}\n"
     ]
    }
   ],
   "source": [
    "# best RMSE score\n",
    "print(\"==== Best RSME Score====\\n\", SVD_Grid.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best RMSE score\n",
    "print(\"\\n==== Hyper-parameter Values for the Best RMSE====\\n\",SVD_Grid.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset RMSE :  0.8470215204103282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Best RSME Score====\n",
      " 0.8470215204103282\n"
     ]
    }
   ],
   "source": [
    "#parameter setting\n",
    "param_grid = {'n_factors': [60], 'n_epochs': [30], 'lr_all': [0.005], 'reg_all': [0.03]}\n",
    "SVD_Grid = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=10, n_jobs=-1)\n",
    "#Fit the data\n",
    "SVD_Grid.fit(train_sdf)\n",
    "print(\"==== Best RSME Score====\\n\", SVD_Grid.best_score['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (RMSE) by the best hyper-paramter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1c27f23f828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the selected parameters1\n",
    "SVD_best = SVD_Grid.best_estimator['rmse']\n",
    "SVD_best.fit(train_sdf.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return print(\"RMSE:\", np.sqrt(((predictions - targets) ** 2).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset RMSE : 0.8412717700600827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8412717700600827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seong\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#prediction for the testset and Check the RMSE\n",
    "pred = [SVD_best.predict(testset.iloc[idx, 0], testset.iloc[idx, 1]).est for idx in range(len(testset))]\n",
    "testset['pred'] = pred\n",
    "rmse(testset['rating'], testset['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the model with the selected parameters\n",
    "SVD_final = SVD_Grid.best_estimator['rmse']\n",
    "SVD_final.fit(ttl_train_sdf.build_full_trainset())\n",
    "\n",
    "#prediction for the testset\n",
    "pred = [SVD_final.predict(df_test_raw.iloc[idx, 0], df_test_raw.iloc[idx, 1]).est for idx in range(len(df_test_raw))]\n",
    "df_test_raw['pred'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>userID</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>1549632</td>\n",
       "      <td>4.192552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2104</td>\n",
       "      <td>428488</td>\n",
       "      <td>4.276588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9410</td>\n",
       "      <td>2069695</td>\n",
       "      <td>3.026097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4005</td>\n",
       "      <td>128311</td>\n",
       "      <td>2.202649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16770</td>\n",
       "      <td>2037731</td>\n",
       "      <td>4.166902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID   userID      pred\n",
       "0     443  1549632  4.192552\n",
       "1    2104   428488  4.276588\n",
       "2    9410  2069695  3.026097\n",
       "3    4005   128311  2.202649\n",
       "4   16770  2037731  4.166902"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_raw.to_csv('Netflix_final_result.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The best model that we found throught the grid search has an RMSE of 0.841 for the validation dataset that was randomly sampled from the netflix_train.txt dataset(20% of the original train dataset). Compared to the Netflix winner who built a regression model with the RMSE of 0.842, the result is a bit lower. By doing so, we understood how powerful the SVD method could be when used for the recommendation system. In addition, the matrix decomposition can be applied with other machine learing algorithms in ensemble ways. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
