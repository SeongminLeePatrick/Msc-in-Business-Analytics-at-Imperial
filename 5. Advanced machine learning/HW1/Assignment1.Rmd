---
title: "Assignment 1"
author: "Seongming Lee (01247436), Yuxuan Luo (01376247) and Nina Hauser (01418616)"
output: pdf_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, fig.align = "center", fig.width=5, fig.height=3)
library(dplyr)
library(ggplot2)
library(stats)
library(caret)
library(formattable)
library(pROC)
library(glmnet)
library(texreg)
data <- read.csv('Tahoe_Healthcare_Data.csv')
```

#Question 1

(1) A linear separation could use $\phi(x)=||x||_2$, since the inner class (black rectangle) has a strictly smaller absolute value than the outer class (blue rectangle).

(2) The two dimension graph can be transformed to a one dimension by using $\phi(x)=x_1*x_2$. As the blue class consists of $x_1$ and $x_2$ values with different signs, it will have strictly negative values, while the black class with equal signs for $x_1$ and $x_2$ has strictly positive values.

(3) Using $\phi_1(x_1)=x_1$,$\phi_2(x_1)=x_1^2$, a parabola can be used to separate the two classes. Consequently, blue class values will now be strictly bigger than black class values and the classes can therefore be split successfully. 


#Question 2

```{r}
readmission_perc <- sum(data$readmit30)/nrow(data)

#no implementation costs
cost_without <- readmission_perc*4382*8000

#costs of CareTracker for all patients
cost_with<- readmission_perc*4382*8000*0.6 + 1200*4382
cost_diff1 <- cost_without - cost_with

#costs of CareTracker for specific patients
cost_specific<- readmission_perc*4382*8000*0.6 + 1200*readmission_perc*4382
cost_diff2 <- cost_without - cost_specific
```

##1. Cost of Readmissions

`r 100*round(readmission_perc,2)` percent of the patients were readmitted in the timespan of one year. As the loss in Medicare reimbursements are estimated to be 8,000 USD per readmitted patient, the values of 7,984,000 USD is derived by the formula:

$$Cost \ without \ CareTracker= Number \ of \ Patients \ * Share \ of \ Readmissions \ * \ Cost \ of \ Readmission $$
$$= 382 \ * 0.2277 * 8000 = 7,984,000$$


##2. Cost of Caretaker

Caretaker reduces the likelihood of readmissions by 40 percent, while costing 1,200 USD per patient. Subsequently, 4,790,400 USD lost by medical reimbursements remain and program costs add up to 5,258,400 USD. Based on the cost analysis in the prior questions, CareTracker would actually increase costs for Tahoe. Consequently, Tahoe should not implement CareTracker for all AMI patients.

$$Cost \ with \ Caretracker = Cost \ of \ Readmissions \ + Program \ Costs$$
$$= 4,790,400 + 5, 258,400 = 10,048,800$$

$$Cost \ Difference = Cost \ without \ Caretracker \ - Cost \ with \ CareTracker$$
$$= 7,984,000 - 10,048,800 = -2,064,800$$


##3. Cost for patient-specific use of CareTracker

With a perfect classifier, Tahoe could reduce costs as it would on give CareTracker do those patients who would be readmitted in the near future. The 1,200 USD implementations costs are therefore only applicable to the correctly identified `100*round(readmissions,2)` share of patients. The upper savings bond is `cost_diff2` USD.
$$New\ Cost \ with \ Caretracker = Cost \ of \ Readmissions \ + Program \ Costs $$
$$= 4,790,400 + 1,197,600 = 5,988,000$$
$$Cost \ Difference = Cost \ without \ Caretracker \ - New \ Cost \ with \ CareTracker$$
$$= 7,984,000 - 5,988,000 = 1,996,000$$


##4. A Simple Classification Algorithm

The best value for S* is 41 with savings of 136,800 USD. 

```{r}
cost_matrix1 <- cbind(S = rep(1,76), Cost = rep(1,76))
i = 1

for (s in seq(25, 100, by = 1)) {
  predict_simple <- ifelse(data$severity.score > s, 1, 0) #predict values
  confusion_simple <- as.matrix(confusionMatrix(predict_simple, data$readmit30)) #create confusion matrix
  savings <- cost_without - ((confusion_simple[2,1] + confusion_simple[2,2])*1200 + (0.6*confusion_simple[2,2] +
                                                                                       confusion_simple[1,2])*8000)
  cost_matrix1[i,1] <- s
  cost_matrix1[i,2] <- savings
  i = i + 1
}
 
plot(cost_matrix1[,1], cost_matrix1[,2], xlab = "Severity Score Threshold S*", ylab = "Savings",type = "l", main = "Cost Plot for Severity Score Classifier")

S <- cost_matrix1[match(max(cost_matrix1[,2]), cost_matrix1[,2])]
savings <- max(cost_matrix1[,2])
kable(cbind("S*" = S , "Savings" = savings), caption = "Best Simple Classifier")
```


##5. A Sophisticated Classification Algorithm

The GLM classifier predicts readmission for any probability equal or bigger than 0.5. 80.35 percent of the observations are accurately classified, with only 3.93 percent of cases where a high risk of readmission is missed and 15.72 percent observations that are given CareTracker without cause. 

```{r}
set.seed(10)
data <- data[sample(nrow(data)),]

glm.fits <- glm(readmit30 ~ ., data = data, family = binomial)
predict2 <- ifelse(predict(glm.fits, data[,1:6], type="response")>=0.5, 1, 0)
kable(confusionMatrix(data$readmit30, predict2)$table, caption = "Confusion Matrix")
```


#6. Cost Savings with Sophisticated Algorithm

The best value for p* is 0.4 with savings of 495,200 USD. 

```{r}
cost_matrix2 <- cbind(p = rep(1,81), Cost = rep(1,81))
i = 1
for (p in seq(0.1, 0.9, by = 0.01)) {
  predict_advanced <- ifelse(predict(glm.fits, data = data[,1:6], type="response") > p, 1, 0)
  confusion_advanced <- as.matrix(confusionMatrix(predict_advanced, data$readmit30)) #create confusion matrix
  savings <- cost_without - ((confusion_advanced[2,1] + confusion_advanced[2,2])*1200 + (0.6*confusion_advanced[2,2] +
                                                                                       confusion_advanced[1,2])*8000)
  cost_matrix2[i,1] <- p
  cost_matrix2[i,2] <- savings
  i = i + 1
}

plot(cost_matrix2[,1], cost_matrix2[,2], xlab = "Probabiliy Threshold p*", ylab = "Savings",type = "l", main = "Cost Plot for GLM Classifier")

p <- cost_matrix2[match(max(cost_matrix2[,2]), cost_matrix2[,2])]
savings2 <- max(cost_matrix2[,2])
kable(cbind("S*" = p, "Savings" = savings2), caption = "Best Advanced Classifier")
```

Comparing the simple (red) to the GLM classifier (black), it becomes visible that the GLM clearly exceeds the performance of the simple classifier. 
```{r}
glm.fits <- glm(readmit30 ~ ., data = data)
predict_S <- ifelse(data$severity.score > 41, 1, 0)
predict_p <- predict(glm.fits, data = data[,1:6])
roc1 <- roc(data$readmit30, predict_S)
roc2 <- roc(data$readmit30, predict_p)
plot(roc1, legacy.axes = TRUE, col = "red", main = "ROC Curves", xlab = "False Positives", ylab = "True Positives", xlim = c(1,0), ylim = c(0,1))
lines(roc2, xlim = c(1,0), ylim = c(0,1))
```

\newpage

```{r, results = "asis"}
texreg(list(glm.fits), caption = "GLM model")
```

\newpage


#Question 3

```{r}
smp_size <- floor(0.70 * nrow(data))

## set the seed to make your partition reproductible
set.seed(123)
train_ind <- sample(seq_len(nrow(data)), size = smp_size)

train <- data[train_ind, ]
test <- data[-train_ind, ]

train_x <- train %>% select(-c(readmit30))
train_y <- train$readmit30

test_x <- test %>% select(-c(readmit30))
test_y <- test$readmit30

lambdas <-seq(0, 1, by = 0.0001)
n <- 1
train_rss <- list()
test_rss <- list()
variance <- list()

for (i in lambdas){
  fit <- glmnet(as.matrix(train_x), train_y, alpha = 0, lambda = i)
  fit_value_train <- predict(fit, newx=as.matrix(train_x))
  train_rss[[n]] <- sum((fit_value_train - train_y)^2)
  
  fit_value_test <- predict(fit, newx=as.matrix(test_x))
  test_rss[[n]] <- sum((fit_value_test - test_y)^2)
  
  variance[[n]] <- var(fit_value_test)
  
  n <- n+1
}
```


(a) With an increasing penalty value $\lambda$, the training RSS will steadily increase. By increasing $\lambda$, the parameter estimates will all be shrunk to zero, deviating from non-regularized model estimates and thus bringing a steady increase in training RSS.

(b) For test RSS, it will decrease initially, and then start increasing in a U shape. Since coefficients estimates are forced to decrease in the beginnging, the test RSS will decrease slightly as the model is less overfitting. However, if $\lambda$ keeps increasing, some necessary coefficients will be shrunk to 0 and removed from the model, leading to an increase in the test RSS.

(c) Variance will decrease steadily. Since large coefficients generally have more variability than smaller coefficients, adding the penalization term reduces variance.

(d) Bias will steadily increase. Higher values of the penalty parameter $\lambda$ constrains parameter estimates, and thus a regularized model with higher $\lambda$ will be more biased compared to non-regularized models.

(e) The irreducible error remains constant, since it is independant of the model, and consequently independant of the value of $\lambda$.

```{r,fig.height=3}
par(mfrow=c(1, 3))
plot( lambdas, train_rss, type="l", col="red" )
plot( lambdas, test_rss, type="l", col="blue" )
plot( lambdas, variance, type="l", col="red" )
```