---
title: "Assignment 3"
author: "Seongming Lee (01247436), Yuxuan Luo (01376247) and Nina Hauser (01418616)"
output:
  pdf_document: default
  html_document:
    code_folding: hide
    prettydoc::html_pretty:
    theme: united
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
---
## Question 1

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
setwd("~/00. Spring/00. Advanced Machine Learning/Assignments/3. assignment")
library(R.matlab)
library(plotrix)

data = readMat("./olivettifaces.mat") # Have it as a .mat file
faces = data$faces
dim(faces)
NumFaces = length(faces[1,])
NumPixels = length(faces[,1]) 
```

```{r,include=FALSE}
NumFacesPCA = 400;  # Number of images we will use for PCA
set.seed(1)
index = sample(1:NumFaces, NumFacesPCA);
TrainImages = faces[,index]
```


```{r,include=FALSE}
n = 2
par(mfrow = c(n,n),     # 2x2 layout
    oma = c(0, 0, 0, 0), # controls rows for text at outer  margins
    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row
    xpd = NA)  
for (i in 1:n^2){
  Face = matrix(TrainImages[,i],sqrt(NumPixels),byrow=FALSE)
  color2D.matplot(Face,axes=FALSE)
}
```

```{r}
pca <- prcomp(t(TrainImages), scale = TRUE)
eigenvector <- pca$rotation
```

To calculate principe components, we first need to transform the faces data(A=4096 x 400)  to  covariance matrix(400 x 400), and then compute eigenvalue and eigenvector from the covariance matrix. And each priciple component, which represents the significance of each face, can be calculated by eigenvectors of the covariance matrix multiplying the face matrix. Hence, each face $\phi_i$ in the training set can be represented as a linear combination of K eigenvectors, formulated as $\phi_i=\mu+\sum_{k=1}^{k}{}\lambda_{1:k}v_{1:k}$ where $\mu$ is the average face, $v$ is the eigenvectors(eigenfaces), and $\lambda$ is eigenvalue. As K keeps increasing to add up more principle components, the explained variance ratio($\frac{\sum_{i=1}^k\lambda_i}{\sum_{i=1}^d\lambda_i}$) is steadliy rising up to 1, which means faces are getting closer to their real faces.

```{r}
n = 2
par(mfrow = c(n,n),     # 2x2 layout
    oma = c(0, 0, 0, 0), # controls rows for text at outer  margins
    mar = c(1, 1, 0, 0), # space for one row of text at ticks and to separate plots
    mgp = c(2, 1, 0),    # axis label at 2 rows distance, tick labels at 1 row
    xpd = NA)  
for (k in c(3,10,25,50)){
  for(i in c(1,2,3,4)) {
  restr <- t(pca$center + pca$scale * pca$rotation[,1:k] %*% t(pca$x[,1:k]))
  restr_Face = matrix(restr[i,],sqrt(NumPixels),byrow=FALSE)
  color2D.matplot(restr_Face,axes=FALSE)
  }
  cat('With k = ',k)
  cat('\nExplained Variance Ratio: ', sum(pca$sdev[1:k]^2)/sum(pca$sdev^2),'\n')
}
```

\newpage
## Question 2
(a) Generate a simulated data set (60*50).
```{r, echo=TRUE}
x <- rbind(matrix(rnorm(20*50, mean = 1), nrow = 20), 
           matrix(rnorm(20*50, mean=2), nrow = 20), 
           matrix(rnorm(20*50, mean=3), nrow = 20))
print(dim(x))
```

(b) Perform PCA on the 60 observations and plot the first two principal component score vectors.
```{r}
pca = prcomp(x)
x.pca = pca$x
plot(x.pca[,1:2], col=c(rep(1,20), rep(2,20), rep(3,20)))
```

(c) The K-means obtained clusters are perfectly assigned to the true class labels.
```{r}
set.seed(123)
res = kmeans(x, centers = 3)
true_class = c(rep(1,20), rep(2,20), rep(3,20)) 
table(res$cluster, true_class)
```

(d) One whole class is assigned to a wrong class, while other classes are classified correctly. This is reasonable since the third cluster is generated with a mean shift of 3, while the mean shift for the second cluster is 2 and for the first class is 1.
```{r}
set.seed(123)
res = kmeans(x, centers = 2)
true = c(rep(1,20), rep(2,20), rep(3,20))
table(res$cluster, true_class)
```

(e) K-means clustering correctly classifys two clusters, but the other class is divided into two classes.
```{r}
set.seed(123)
res = kmeans(x, centers = 4)
true = c(rep(1,20), rep(2,20), rep(3,20)) 
table(res$cluster, true_class)
```

(f) Same result with (b), as the first two principal component score vectors carries enough information to cluster correctly.
```{r}
set.seed(123)
res = kmeans(x.pca[,1:2], centers = 3) 
pr.var=pca$sdev^2
pve = pr.var/sum(pr.var)
true = c(rep(1,20), rep(2,20), rep(3,20)) 
table(res$cluster, true_class)
```

(g) Same with (b), scaling does not change the results, since original data is generated by rnorm() with mean shifts and standard deviation of 1, and scaling is simply transforming original data into data with mean of 0 and standard deviation of 1.
```{r}
set.seed(123)
res = kmeans(scale(x), centers = 3)
true = c(rep(1,20), rep(2,20), rep(3,20)) 
table(res$cluster, true_class)
```

