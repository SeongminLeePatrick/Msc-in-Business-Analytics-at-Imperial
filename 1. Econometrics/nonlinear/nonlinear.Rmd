---
title: "Logit and Probit Models"
author: "Jiahua Wu"
subtitle: BS1802 Statistics and Econometrics
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(car)
library(dplyr)
library(stargazer)
library(lmtest)
```

## Example 17.1 (slides 16-17)
The function to estimate logit and profit models is $glm$. When we estimate logit (probit), we need specify $family = ``binomial"$ (as the response follows a Bernoulli distribution, which is a special case of binomial distribution), and $link=``logit"$ for logit model and $link=``probit"$ for probit model.

```{r, results='asis'}
# Example 17.1
load("mroz.RData")

# estimation of binary response models
inlf.lpm <- glm(inlf ~ nwifeinc + educ + exper + expersq + age 
                + kidslt6 + kidsge6, family = "gaussian", data)

inlf.lpm.lm <- lm(inlf ~ nwifeinc + educ + exper + expersq + age 
                  + kidslt6 + kidsge6, data)

inlf.probit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age 
                   + kidslt6 + kidsge6, family = "binomial"(link = "probit"), data)

inlf.logit <- glm(inlf ~ nwifeinc + educ + exper + expersq + age
                  + kidslt6 + kidsge6, family = "binomial"(link = "logit"), data)
stargazer(inlf.lpm.lm, inlf.lpm, inlf.probit, inlf.logit, header = FALSE, type = 'latex', 
          title = "Example 17.1. Labour Force Participation")
```

```{r}
summary(inlf.logit)
```
First, let us try to understand the output of $glm$. We take logit model as an example. The first thing we observe from the summary is deviance residuals. There are five different types of residuals from $glm$ models. The most commonly used one, and the one reported in summary by default is deviance residuals.  The sum of squared of devianceresiduals equals to the deviance of the estimated model. We can verify this by coding it manually as follows.
```{r}
# deviance = sum of squared deviance residuals
summary(residuals(inlf.logit, type = "deviance"))
sum(residuals(inlf.logit, type = "deviance")^2)
inlf.logit$deviance

# deviance = -2 * log likelihood
(-2) * logLik(inlf.logit)
```
The deviance of the model itself equals to -2 times the log-likelihood of the model. We can think of deviance as the couterpart of sum of squared residuals in the multiple regression models. We want to minimize deivances for logit and profit models (equivalent to maximizing log likelihood). The deviance residual sort of measures the fit for each data point, just like residuals in multiple regression.

The interpretation of coefficients, standard errors and etc. is the same as that in linear regression models. One slight difference is that, instead of calling it $t$ test, we refer to it as $z$ test, as now with logit and probit models, the test statistic follows a standard normal distribution.

Dispersion parameter is not really useful for logit/probit models. It is mainly used in Poisson models to allow for different mean and variance (theoretically, mean and variance is always the same for Poisson distributed random variables). In practice, for any count data, we can well observe that mean and variance is different, and this dispersion parameter allows a more flexbile model to fit the data.

Lastly, the maximum likelihood estimates shall be estimated by solving an optimization problem. The underly algorightm is an iterative algorithm (Newton-Raphson algorithm). "Number of Fisher Scoring iterations" indicates the number of iterations the algorithm runs to find the optimal values.

In the class, we discussed several different measures of goodness of fit. The following code shows how to calculate each of them. 
```{r}
### goodness of fit
# pseudo r-squared
1 - inlf.logit$deviance/inlf.logit$null.deviance

# percent correctly predicted [sum(abs(data$inlf - inlf.logit$fitted.values) <= 0.5): fitting이 잘 된 수)]
sum(abs(data$inlf - inlf.logit$fitted.values) <= 0.5)/nrow(data)


# information criteria
AIC(inlf.logit)
BIC(inlf.logit)
```

When it comes to hypothesis tesing, there are two functions we can use, one is $lrtest$ in the $lmtest$ package, and the other one is $linearHypothesis$ in the $car$ package. The implementations of both of them are based on likelihood ratio test. We reject null when test statistic is sufficiently large ($p$ value is sufficiently small).
```{r}
# test for overall significance
lrtest(inlf.logit)

# test of overall significance manually
1 - pchisq(inlf.logit$null.deviance - inlf.logit$deviance, 
           df = inlf.logit$df.null - inlf.logit$df.residual)

# hypothesis testing
linearHypothesis(inlf.logit, c("exper = 0", "expersq = 0"))
```

Similar to multiple regression models, we can use $confint$ for confidence intervals. By default, the function produces 95% confidence intervals for all model coefficients.
```{r}
# confidence intervals
confint(inlf.logit)
```

$step$ function still works for probit and logit, and the interpretation of output would be exactly the same as that in linear regressions.
```{r, eval=FALSE}
### model selection
inlf.null <- glm(inlf ~ 1, family = "binomial"(link = "logit"), data)
inlf.full <- glm(inlf ~ nwifeinc + educ + exper + expersq + age
                + kidslt6 + kidsge6, family = "binomial"(link = "logit"), data)
step(inlf.null, scope = list(lower = inlf.null, upper = inlf.full), direction = "forward")
step(inlf.full, direction = "backward")
```

$predict$ functions comes with two types, link or response. When $type = ``link"$, it returns the linear combination of independent variables, i.e., $\hat{\beta}_0+\hat{\beta}_1x_1+\ldots +\hat{\beta}_k x_k$. Alternatively, when $type = ``response"$, it returns the predicted probability of $y=1$. We also code it manually to verify.
```{r}
### prediction 
new.ob = data.frame(nwifeinc = 10.91, educ = 12, exper = 14, expersq = 14^2,
                     age = 32, kidslt6 = 1, kidsge6 = 0)
predict(inlf.logit, newdata = new.ob, type = "link")
predict(inlf.logit, newdata = new.ob, type = "response")

# manual verification
xb <- sum(cbind(1, new.ob) * inlf.logit$coefficients); xb
phat <- exp(xb)/(1 + exp(xb)); phat
```

Lastly, let us discuss the interpretation of model. Again using logit model as example, the estimated coefficients no longer measure the partial effects of corresponding variables, as the partial effect of any independent variable now depends on values of all independent variables (slide 9). In particular, for logit model, $g(z) = G(z)\cdot [1-G(z)]$. Consequently, we can calculate average partial effect for $x_j$ in a logit model using the formula $$n^{-1}\sum_{i=1}^n G(\hat{\beta}_0+\hat{\beta}_1x_{i1}+\cdots+\hat{\beta}_k x_{ik})\cdot [1-G(\hat{\beta}_0+\hat{\beta}_1x_{i1}+\cdots+\hat{\beta}_k x_{ik})]\hat{\beta}_j,$$ where $G(z)=\frac{\exp(z)}{1+\exp(z)}$. The code to calcuate the average partial effect is given below.
```{r}
# partial effects in logit model
mean(inlf.logit$fitted.values * (1 - inlf.logit$fitted.values)) * inlf.logit$coefficients
```
If we compare them against estimated coefficients from the linear probability model (in Table 1), we will find them quite similar. This is typically the case in practice - coeffients from the linear probablity model usually give reliable estimates of average partial effects of independent variables in the model.

One key difference between LPM and logit/probit is that the marginal effect is assumed to be constant with LPM, however, it is no longer the case for logit/probit. That is, with LPM, an extra kid always decreases the chance of labor force participation by 0.26, regardless how many kids the individual already has. However, intuitively, we would think that the first kid has the greatest effect, and the marginal impact of an extra kid shall decrease. This nonlinearity is reflected by logit/probit output. We can calculate the impact of having the first kid less than 6, and the second kid less than 6, using the code below. It turns out the first kid reduces the probability of being in the labor force by 0.35, while the effect of the second kid is around 0.22. This nonlinearity allows logit/probit fit the data better, as reflected by log likelihood shown in Table 1.
```{r}
# non-linear partial effects
new.ob <- with(data, data.frame(nwifeinc = mean(nwifeinc), educ = mean(educ), exper = mean(exper),
                    expersq = mean(expersq), age = mean(age), kidslt6 = c(0, 1, 2), kidsge6 = 1))

# partial effect of having the first kid less than 6
predict(inlf.logit, newdata = new.ob[2, ], type = "response") - 
        predict(inlf.logit, newdata = new.ob[1, ], type = "response")
# partial effect of having the second kid less than 6
predict(inlf.logit, newdata = new.ob[3, ], type = "response") - 
        predict(inlf.logit, newdata = new.ob[2, ], type = "response")
```